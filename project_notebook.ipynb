{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MA513 - Hands-on Machine Learning for Cybersecurity PROJECT** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce projet nous allons dévelloper un modèle NER. Chaque étape aura sa propres explications et nous justifirons chaque choix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imporation des libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, SimpleRNN\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import string\n",
    "from pprint import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode 1 : Création de notre modèle LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 1 : Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette étape, nous allons effectuer un préprocessing des données pour optimiser leur qualité et garantir des performances fiables et robustes des modèles d’apprentissage automatique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Chemin du fichier JSONLines en entrée\n",
    "input_file1 = \"data/NER-TRAINING.jsonlines\"\n",
    "input_file2 = \"data/NER-VALIDATION.jsonlines\"\n",
    "input_file_test = \"data/NER-TESTING.jsonlines\"\n",
    "# Liste pour stocker les données\n",
    "\n",
    "def open_file(input_file):\n",
    "    data = []\n",
    "    # Charger les données JSONLines\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in infile:\n",
    "            record = json.loads(line)\n",
    "            tokens = record[\"tokens\"]\n",
    "            ner_tags = record[\"ner_tags\"]\n",
    "            index = record[\"unique_id\"]\n",
    "            \n",
    "            # Ajouter chaque token, tag et index à la liste\n",
    "            for token, ner_tag in zip(tokens, ner_tags):\n",
    "                data.append({\"index\": index, \"tokens\": token, \"ner_tags\": ner_tag})\n",
    "    return data\n",
    "\n",
    "def open_file2(input_file):\n",
    "    data = []\n",
    "    # Charger les données JSONLines\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in infile:\n",
    "            record = json.loads(line)\n",
    "            tokens = record[\"tokens\"]\n",
    "            index = record[\"unique_id\"]\n",
    "            \n",
    "            # Ajouter chaque token, tag et index à la liste\n",
    "            for token in tokens:\n",
    "                data.append({\"index\": index, \"tokens\": token})\n",
    "    return data\n",
    "# Créer une DataFrame à partir des données\n",
    "#training data\n",
    "data1 = open_file(input_file1)\n",
    "df_train = pd.DataFrame(data1)\n",
    "\n",
    "data2 = open_file(input_file2)\n",
    "df_val = pd.DataFrame(data2)\n",
    "\n",
    "data3 = open_file2(input_file_test)\n",
    "df_test = pd.DataFrame(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet('data_parquet/NER-TRAINING.parquet', index=False)\n",
    "df_val.to_parquet('data_parquet/NER-VALIDATION.parquet', index=False)\n",
    "df_test.to_parquet('data_parquet/NER-TESTING.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(\"data_parquet/NER-TRAINING.parquet\")\n",
    "df_val = pd.read_parquet(\"data_parquet/NER-VALIDATION.parquet\")\n",
    "df_test = pd.read_parquet(\"data_parquet/NER-TESTING.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette étape nous avons choisi d'enregistrer nos données en parquet files car ceci représente plusieurs avantages pour notre volume de données. Meilleurs compressions, plus de flexibilité et donc meilleurs performances. \n",
    "1. ressources : https://medium.com/munchy-bytes/are-you-using-parquet-with-pandas-in-the-right-way-595c9ee7112"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nettoyage de données  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette étape nous allons analyser nos données afin de pouvoir retirer les éléments qui serait en trop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation de notre dataset \n",
    "file_path1 = \"data_parquet/NER-TRAINING.parquet\" #dataset -> training\n",
    "file_path2 = \"data_parquet/NER-VALIDATION.parquet\"\n",
    "file_path3 = \"data_parquet/NER-TESTING.parquet\"\n",
    "\n",
    "df_train = pd.read_parquet(file_path1)\n",
    "df_val = pd.read_parquet(file_path2)\n",
    "df_test = pd.read_parquet(file_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage de nos données : \n",
    "#affichage val\n",
    "print(\"données df_train : \\n \\n \", df_train)\n",
    "print(len(df_train['tokens'].iloc[0]))\n",
    "print(len(df_train['tokens'].iloc[1]))\n",
    "#affichage \n",
    "print(\"données df_train : \\n \\n \", df_val)\n",
    "print(len(df_val['tokens'].iloc[0]))\n",
    "print(len(df_val['tokens'].iloc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant même de commencer nous devons bien comprendre globalement la représentation de chaque colonnes : \n",
    "\n",
    "1. unique_id : un nombre entier \n",
    "2. token : un chaine de charactère, le coeur de notre dataset \n",
    "3. ner_tag : notre label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage des dimmensions avant nettoyage : \n",
    "print(\"dimensions avant nettoyage : \", df_train.shape)\n",
    "print(\"dimensions avant nettoyage : \", df_val.shape)\n",
    "print(\"dimensions avant nettoyage : \", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons remarqué que notre dataset contient de nombreuses valeurs inutiles qui pourraient nuire à l'entraînement de notre modèle. Par exemple, les caractères spéciaux et les majuscules, qui sont peu pertinents dans ce contexte. Dans cette étape, nous allons procéder à leur suppression afin d'optimiser la qualité des données pour l'entraînement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on retire les ponctuations : \n",
    "df_train = df_train[~df_train['tokens'].isin(list('?$#@./|:()\",;[]{}-'))]\n",
    "print('dimensions après nettoyage : ', df_train.shape)\n",
    "\n",
    "df_val = df_val[~df_val['tokens'].isin(list('?$#@./|:()\",;[]{}-'))]\n",
    "print('dimensions après nettoyage : ', df_val.shape)\n",
    "\n",
    "df_test = df_test[~df_test['tokens'].isin(list('?$#@./|:()\",;[]{}-'))]\n",
    "print('dimensions après nettoyage : ', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction adaptée pour gérer uniquement 'unique_id'\n",
    "def index_mapper(df):\n",
    "    \n",
    "    unique_ids = df['index'].unique()\n",
    "    index = np.arange(1, len(unique_ids) + 1, 1)\n",
    "    index_dict = dict(zip(unique_ids, index))\n",
    "    df['index'] = [index_dict[uid] for uid in df['index']]\n",
    "    df.set_index('index', inplace=True)\n",
    "    return df\n",
    "\n",
    "df_train = index_mapper(df_train)\n",
    "df_val = index_mapper(df_val)\n",
    "df_test = index_mapper(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous sélectionnons maintenant uniquement les valeurs pertinentes de notre DataFrame, afin d'entraîner notre modèle de manière optimale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ = df_train[['tokens', 'ner_tags']]\n",
    "df_train_.head()\n",
    "\n",
    "df_val_ = df_val[['tokens', 'ner_tags']]\n",
    "df_val_.head()\n",
    "\n",
    "df_test_ = df_test\n",
    "df_test_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des données \n",
    "Dans cette étape nous essayer d'analyser les données afin d'éléminer les possible outliners qui pourrais fausser notre modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot length of the sentences.\n",
    "index, length = np.unique(df_train_.index, return_counts=True)\n",
    "fig, ax = plt.subplots(figsize=[25,6])\n",
    "N, bins, patches = ax.hist(length, bins=100)\n",
    "plt.xlabel('Texte long')\n",
    "plt.ylabel('Freq')\n",
    "plt.show()\n",
    "\n",
    "print('Nombre de phrase dans notre dataset : ', len(length))   ## Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la répartition des ner_tags dans df_train\n",
    "train_tag_distribution = df_train['ner_tags'].explode().value_counts()\n",
    "\n",
    "# Calculer la répartition des ner_tags dans df_val\n",
    "val_tag_distribution = df_val['ner_tags'].explode().value_counts()\n",
    "\n",
    "# Afficher les distributions\n",
    "print(\"Répartition des ner_tags dans le dataset d'entraînement :\\n\", train_tag_distribution)\n",
    "print(\"\\nRépartition des ner_tags dans le dataset de validation :\\n\", val_tag_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 2 : Entraînement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encodage du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df_train_['ner_tags'] = le.fit_transform(df_train_['ner_tags'])\n",
    "df_train_.head()\n",
    "df_val_['ner_tags'] = le.fit_transform(df_val_['ner_tags'])\n",
    "df_val_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Because padded variable has to be kept as 0 and not -1. Else label encode would trouble it!\n",
    "df_train_.ner_tags += 1\n",
    "df_train_.head()\n",
    "df_val_.ner_tags += 1\n",
    "df_val_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization du data training \n",
    "Dans cette étape, nous allons effectuer une tokenisation des données pour les segmenter en unités plus petites (comme des mots ou des phrases), afin de faciliter leur traitement par les modèles d’apprentissage automatique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='?$#@./|:()\",;[]{}-',\n",
    "                                                  lower=False, \n",
    "                                                  split=' ',\n",
    "                                                  num_words=2000,\n",
    "                                                  oov_token='')       # Initialize\n",
    "\n",
    "tokenizer.fit_on_texts(df_train_.tokens.values) \n",
    "tokenizer.fit_on_texts(df_val_.tokens.values) \n",
    "tokenizer.fit_on_texts(df_test.tokens.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to numeric\n",
    "tokens_train  = tokenizer.texts_to_sequences(df_train_.tokens.values)\n",
    "tokens_val  = tokenizer.texts_to_sequences(df_val_.tokens.values)\n",
    "tokens_test  = tokenizer.texts_to_sequences(df_test.tokens.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_token = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verification de la tokenization\n",
    "df_train_.tokens = tokens_train\n",
    "df_train_.head()\n",
    "df_val_.tokens = tokens_val\n",
    "df_val_.head()\n",
    "df_test.tokens = tokens_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les éléments vides des données d'entraînement\n",
    "import numpy as np\n",
    "\n",
    "# Filtrer les tokens non vides (reste sous forme de liste Python)\n",
    "b = [len(token) > 0 for token in tokens_train]  \n",
    "df_train_ = df_train_.iloc[b, :]  # Filtrer le DataFrame\n",
    "tokens_train = [tokens_train[i] for i, keep in enumerate(b) if keep]  # Filtrer les tokens\n",
    "\n",
    "#\n",
    "b = [len(token) > 0 for token in tokens_val]  \n",
    "df_val_ = df_val_.iloc[b, :]  # Filtrer le DataFrame\n",
    "tokens_val = [tokens_val[i] for i, keep in enumerate(b) if keep]  # Filtrer les tokens\n",
    "\n",
    "b = [len(token) > 0 for token in tokens_test]  \n",
    "df_test = df_test.iloc[b, :]  # Filtrer le DataFrame\n",
    "tokens_test = [tokens_test[i] for i, keep in enumerate(b) if keep]  # Filtrer les tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_.tokens = df_train_.tokens.apply(lambda x: x[0])\n",
    "print(df_train_.head())\n",
    "\n",
    "df_val_.tokens = df_val_.tokens.apply(lambda x: x[0])\n",
    "print(df_val_.head())\n",
    "\n",
    "df_test.tokens = df_test.tokens.apply(lambda x: x[0])\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ = df_train_.astype(str)\n",
    "df_train_concat = df_train_.groupby(df_train_.index).agg(lambda x: ' '.join(x))\n",
    "print(df_train_concat.head())\n",
    "\n",
    "df_val_ = df_val_.astype(str)\n",
    "df_val_concat = df_val_.groupby(df_val_.index).agg(lambda x: ' '.join(x))\n",
    "print(df_val_concat.head())\n",
    "\n",
    "df_test = df_test.astype(str)\n",
    "df_test_concat = df_test.groupby(df_test.index).agg(lambda x: ' '.join(x))\n",
    "print(df_test_concat.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train_concat.tokens = df_train_concat.tokens.apply(lambda x: x.split(' '))\n",
    "df_train_concat.ner_tags = df_train_concat.ner_tags.apply(lambda x: x.split(' '))\n",
    "print('Training Set Shape after Concatenating Sentences: ', df_train_concat.shape)\n",
    "\n",
    "df_val_concat.tokens = df_val_concat.tokens.apply(lambda x: x.split(' '))\n",
    "df_val_concat.ner_tags = df_val_concat.ner_tags.apply(lambda x: x.split(' '))\n",
    "print('Training Set Shape after Concatenating Sentences: ', df_val_concat.shape)\n",
    "\n",
    "df_test_concat.tokens = df_test_concat.tokens.apply(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Data for Model Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = df_train_concat.tokens\n",
    "train_tags = df_train_concat.ner_tags\n",
    "\n",
    "val_sentences = df_val_concat.tokens\n",
    "val_tags = df_val_concat.ner_tags\n",
    "\n",
    "test_sentences = df_test_concat.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in test_sentences:\n",
    "    for token in sentence:  # Parcourt les tokens dans chaque sous-liste\n",
    "        if not str(token).isdigit():  # Convertit le token en chaîne avant de vérifier\n",
    "            print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [list(map(int, sentence)) for sentence in train_sentences]\n",
    "train_tags = [list(map(int, sentence)) for sentence in train_tags]\n",
    "\n",
    "val_sentences = [list(map(int, sentence)) for sentence in val_sentences]\n",
    "val_tags = [list(map(int, sentence)) for sentence in val_tags]\n",
    "\n",
    "test_sentences = [list(map(int, sentence)) for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Code\n",
    "batch_size = 16\n",
    "train_shuffle_buffer_size = len(train_sentences)\n",
    "validation_shuffle_buffer_size = len(val_sentences)\n",
    "\n",
    "# Fill the required cells to complete the function\n",
    "def transform_pad(input, output):\n",
    "    input  = input.to_tensor(default_value=0, shape=[None, None])\n",
    "    output = output.to_tensor(default_value=0, shape=[None, None])   \n",
    "    return input, output\n",
    "\n",
    "def transform_pad2(input):\n",
    "    input  = input.to_tensor(default_value=0, shape=[None, None])\n",
    "    #output = output.to_tensor(default_value=0, shape=[None, None])   \n",
    "    return input\n",
    "\n",
    "train_processed_x = tf.ragged.constant(train_sentences)\n",
    "validate_processed_x = tf.ragged.constant(val_sentences)\n",
    "test_processed_x = tf.ragged.constant(test_sentences)\n",
    "\n",
    "train_processed_y = tf.ragged.constant(train_tags)\n",
    "validate_processed_y = tf.ragged.constant(val_tags)\n",
    "\n",
    "# Create TF Dataset\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_processed_x, train_processed_y))\n",
    "validation_data = tf.data.Dataset.from_tensor_slices((validate_processed_x, validate_processed_y))\n",
    "test_data = tf.data.Dataset.from_tensor_slices(test_processed_x)\n",
    "\n",
    "#############\n",
    "# Train data\n",
    "#############\n",
    "train_data = train_data.shuffle(buffer_size=train_shuffle_buffer_size)\n",
    "train_data = train_data.batch(batch_size)\n",
    "train_data = train_data.map(transform_pad, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_data = train_data.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "##################\n",
    "# Validation data\n",
    "##################\n",
    "validation_data = validation_data.batch(batch_size)\n",
    "validation_data = validation_data.map(transform_pad, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "validation_data = validation_data.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "##################\n",
    "# Test data\n",
    "##################\n",
    "test_data = test_data.batch(batch_size)\n",
    "test_data = test_data.map(transform_pad2, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_data = test_data.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "print(\"train_data\", train_data)\n",
    "print(\"validation_data\", validation_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etape 3 : Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "hidden_size_LSTM = 100\n",
    "hidden_size_Dense = 100\n",
    "\n",
    "def build_model():\n",
    "\n",
    "  ## ---------------------------------------------------------------------------\n",
    "  ## FORWARD LAYER -------------------------------------------------------------\n",
    "  ## ---------------------------------------------------------------------------\n",
    "\n",
    "  ## Define input layer.\n",
    "  inputs_f = tf.keras.Input(shape=[None])\n",
    "\n",
    "\n",
    "  ## Embedding Layer for forward.\n",
    "  embedding_layer_f = tf.keras.layers.Embedding(input_dim=2000,\n",
    "                                                output_dim=300,\n",
    "                                                # weights=[embedding_matrix],\n",
    "                                                # input_length=max_length,\n",
    "                                                trainable=True,\n",
    "                                                mask_zero=True)\n",
    "\n",
    "  ## Create a forward LSTM.\n",
    "  RNN1_layer_f = tf.keras.layers.LSTM(hidden_size_LSTM, return_sequences=True)\n",
    "\n",
    "  ## Add Dropout after first LSTM\n",
    "  dropout1_f = tf.keras.layers.Dropout(0.3)  ## Adjust dropout rate as needed.\n",
    "\n",
    "  ## Create a dense layer for simulating the highway layer rather than using it here.\n",
    "  dense_layer_f = tf.keras.layers.Dense(units=hidden_size_Dense, activation='linear', use_bias=False)\n",
    "\n",
    "  ## Add Dropout after Dense layer\n",
    "  dropout2_f = tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "  ## Create an additive layer.\n",
    "  additive_layer_f = tf.keras.layers.Add()\n",
    "\n",
    "  ## Create second forward LSTM.\n",
    "  RNN2_layer_f = tf.keras.layers.LSTM(hidden_size_LSTM, return_sequences=True)\n",
    "\n",
    "  ## Add Dropout after second LSTM\n",
    "  dropout3_f = tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "  ## Pass Inputs ---------------------------------------------------------------\n",
    "\n",
    "  embedding_f = embedding_layer_f(inputs_f)   ## Get forward and backward embeddings.\n",
    "  r_f = dropout1_f(RNN1_layer_f(embedding_f)) ## Get LSTM outputs with dropout.\n",
    "  z_f = dropout2_f(dense_layer_f(embedding_f))## Get Dense layer outputs with dropout.\n",
    "  h_f = dropout3_f(RNN2_layer_f(r_f + z_f))   ## Get LSTM2 outputs with dropout.\n",
    "\n",
    "\n",
    "  ## ---------------------------------------------------------------------------\n",
    "  ## BACKWARD LAYER ------------------------------------------------------------\n",
    "  ## ---------------------------------------------------------------------------\n",
    "\n",
    "  ## Create a forward LSTM.\n",
    "  RNN1_layer_b = tf.keras.layers.LSTM(hidden_size_LSTM, return_sequences=True, go_backwards=True)\n",
    "\n",
    "  ## Add Dropout after first LSTM\n",
    "  dropout1_b = tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "  ## Create a dense layer for simulating the highway layer rather than using it here.\n",
    "  dense_layer_b = tf.keras.layers.Dense(units=hidden_size_Dense, activation=None, use_bias=False)\n",
    "\n",
    "  ## Add Dropout after Dense layer\n",
    "  dropout2_b = tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "  ## Create an additive layer.\n",
    "  additive_layer_b = tf.keras.layers.Add()\n",
    "\n",
    "  ## Create second forward LSTM.\n",
    "  RNN2_layer_b = tf.keras.layers.LSTM(hidden_size_LSTM, return_sequences=True, go_backwards=True)\n",
    "\n",
    "  ## Add Dropout after second LSTM\n",
    "  dropout3_b = tf.keras.layers.Dropout(0.3)\n",
    "\n",
    "  ## Pass Inputs ------------------------------------------------------------\n",
    "\n",
    "  r_b = dropout1_b(RNN1_layer_b(embedding_f)) ## Get LSTM outputs with dropout.\n",
    "  r_b = r_b[:,::-1,:]                         ## We need to reverse the output from go_backwards. Ref: https://medium.com/@rachit1jain/lstm-go-backwards-unravelling-its-hidden-secrets-ed094952b5cc\n",
    "  z_b = dropout2_b(dense_layer_b(embedding_f))## Get Dense layer outputs with dropout.\n",
    "  h_b = dropout3_b(RNN2_layer_b(r_b + z_b))   ## Get LSTM2 outputs with dropout.\n",
    "  h_b = h_b[:,::-1,:]                         ## We need to reverse the output from go_backwards. Ref: https://medium.com/@rachit1jain/lstm-go-backwards-unravelling-its-hidden-secrets-ed094952b5cc\n",
    "\n",
    "\n",
    "  ## ---------------------------------------------------------------------------\n",
    "  ## For Outputs ---------------------------------------------------------------\n",
    "  ## ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "  ## Create Softmax Layer.\n",
    "  softmaxLayer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=le.classes_.shape[0]+1, activation='softmax', name='softmaxLayer'))\n",
    "\n",
    "  output_f = softmaxLayer(h_f)\n",
    "  output_b = softmaxLayer(h_b)\n",
    "\n",
    "  output_mf = 0.5*output_f\n",
    "  output_mb = 0.5*output_b\n",
    "  output = output_mf + output_mb\n",
    "\n",
    "  ## ---------------------------------------------------------------------------\n",
    "  ## Setup the Outputs ---------------------------------------------------------\n",
    "  ## ---------------------------------------------------------------------------\n",
    "\n",
    "  ## Set up the model with appropriate inputs and the output defined above \n",
    "  model = tf.keras.Model(inputs=inputs_f, outputs=output, name='Model')\n",
    "\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################\n",
    "# Training Params\n",
    "############################\n",
    "\n",
    "import time\n",
    "\n",
    "learning_rate = 5e-4\n",
    "epochs = 40\n",
    "\n",
    "# Free up memory\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Build the model\n",
    "model = build_model()\n",
    "\n",
    "# Print the model architecture\n",
    "print(model.summary())\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "# Callbacks\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='model_2.{epoch:02d}-{loss:.2f}.h5.keras'),\n",
    "]\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "              loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    "\n",
    "training_results = model.fit(\n",
    "        train_data,\n",
    "        epochs=epochs, \n",
    "        verbose=1,\n",
    "        # class_weight=class_weights,\n",
    "        validation_data=validation_data,\n",
    "        callbacks=my_callbacks)\n",
    "\n",
    "execution_time = (time.time() - start_time)/60.0\n",
    "print(\"Training execution time (mins)\",execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trace plot of the loss of the model\n",
    "plt.plot(training_results.history['loss'], label='Train')\n",
    "plt.plot(training_results.history['val_loss'], label='Validation')\n",
    "plt.title('Loss Plot')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trace plot of the loss of the model\n",
    "plt.plot(training_results.history['sparse_categorical_accuracy'], label='Train')\n",
    "plt.plot(training_results.history['val_sparse_categorical_accuracy'], label='Validation')\n",
    "plt.title('Accuracy Plot')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "COMMENTAIRE: \n",
    "ce graphe montre un entraînement efficace mais un début de sur-apprentissage. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Étape 5 : Prédiction \n",
    "Dans cette étape nous allons utiliser notre modèles afin de pouvoir prédire les différents \"ner_tags\" de notre fichier NER-TESTING.jsonlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions sur les données de test\n",
    "predictions = model.predict(test_data)\n",
    "predictions_dense = predictions.to_tensor()\n",
    "predicted_tags = predictions_dense.numpy().argmax(axis=-1)\n",
    "predicted_tags[predicted_tags == 7] = 6\n",
    "\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "le_mapping_new = {le_name_mapping[k]: k for k in le_name_mapping.keys()}\n",
    "\n",
    "# Mapping des indices des classes vers les étiquettes originales\n",
    "predicted_labels = [\n",
    "    [le_mapping_new[tag] for tag in sentence if tag in le_mapping_new]\n",
    "    for sentence in predicted_tags\n",
    "]\n",
    "\n",
    "token_list = []\n",
    "label_list = []\n",
    "tokens_reconstructed = [[tokenizer.index_word[idx] for idx in seq] for seq in test_sentences]\n",
    "print(len(tokens_reconstructed))\n",
    "for i, (tokens, labels) in enumerate(zip(tokens_reconstructed, predicted_labels)):\n",
    "    print(f\"Phrase {i + 1}:\")\n",
    "    for token, label in zip(tokens, labels):\n",
    "        print(f\"{token} -> {label}\")\n",
    "        token_list.append(token)\n",
    "        label_list.append(label)\n",
    "    print(\"-\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du dossier résultat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('token list :', len(token_list))\n",
    "print('label list :', len(label_list)) \n",
    "print(token_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from collections import defaultdict\n",
    "\n",
    "data3 = open_file2(input_file_test)\n",
    "i = 0\n",
    "custom_punctuation = '?$#@./|:()\",;[]{}-'\n",
    "custom_punctuation_list = list(custom_punctuation)\n",
    "#print(custom_punctuation_list)\n",
    "for item in data3:\n",
    "\n",
    "    if item['tokens'] in custom_punctuation_list or item['tokens'] == '...':\n",
    "        item['ner_tag'] = 'O'\n",
    "    else:\n",
    "        if i != 23391: \n",
    "            i += 1\n",
    "            item['ner_tag'] = label_list[i]\n",
    "            #print(\"items :\", item['tokens'])\n",
    "            #print(\"token :\", token_list[i])\n",
    "            #print(i)\n",
    "        else: \n",
    "             item['ner_tag'] = 'O'\n",
    "        \n",
    "\n",
    "# Utilisation de defaultdict pour regrouper les tokens et ner_tags par index\n",
    "grouped_data = defaultdict(lambda: {'tokens': [], 'ner_tags': []})\n",
    "\n",
    "# Remplir les listes tokens et ner_tags pour chaque index\n",
    "for item in data3:\n",
    "    index = item['index']\n",
    "    grouped_data[index]['tokens'].append(item['tokens'])\n",
    "    grouped_data[index]['ner_tags'].append(item['ner_tag'])\n",
    "\n",
    "# Transformer chaque groupe d'index en un format souhaité\n",
    "formatted_data = []\n",
    "for index, value in grouped_data.items():\n",
    "    formatted_data.append({\n",
    "        \"unique_id\": index,\n",
    "        \"tokens\": value['tokens'],\n",
    "        \"ner_tags\": value['ner_tags']\n",
    "    })\n",
    "\n",
    "# Enregistrer le résultat en jsonlines\n",
    "with open('resultat/output.jsonlines', 'w') as f:\n",
    "    for entry in formatted_data:\n",
    "        json.dump(entry, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "# Afficher le résultat pour vérification\n",
    "print(formatted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Méthode 2 : Utilisation d'un modèle pré-entrainer - SciBert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "import warnings\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reading the WIESP-NER_TRAINING Dataset \n",
    "input_file1 = \"data_parquet/NER-TRAINING.parquet\"\n",
    "input_file2 = \"data_parquet/NER-VALIDATION.parquet\"\n",
    "df_train = pd.read_parquet(input_file1)\n",
    "df_val = pd.read_parquet(input_file2)\n",
    "\n",
    "# Renaming the columns as required by simpletransformer train method\n",
    "#df = df[['index', 'tokens', 'ner_tags']]\n",
    "df_train = df_train.rename(columns={'index':'sentence_id', 'tokens':'words', 'ner_tags':'labels'})\n",
    "df_val = df_val.rename(columns={'index':'sentence_id', 'tokens':'words', 'ner_tags':'labels'})\n",
    "df_train.set_index('sentence_id',inplace=True)\n",
    "df_val.set_index('sentence_id',inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Répartition des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la répartition des ner_tags dans df_train\n",
    "train_tag_distribution = df_train['labels'].explode().value_counts()\n",
    "\n",
    "# Calculer la répartition des ner_tags dans df_val\n",
    "val_tag_distribution = df_val['labels'].explode().value_counts()\n",
    "\n",
    "# Afficher les distributions\n",
    "print(\"Répartition des ner_tags dans le dataset d'entraînement :\\n\", train_tag_distribution)\n",
    "print(\"\\nRépartition des ner_tags dans le dataset de validation :\\n\", val_tag_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.head(5))\n",
    "print(df_val.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.ner import NERModel, NERArgs\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# Calcul des poids des classes\n",
    "# Calcul des poids pour chaque classe\n",
    "\n",
    "# Setting model arguments\n",
    "model_args = NERArgs()\n",
    "model_args.labels_list = list(df_train.labels.unique())\n",
    "model_args.num_train_epochs = 5\n",
    "#model_args.class_weights = class_weights.tolist()\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = False\n",
    "model_args.max_seq_length = 256\n",
    "model_args.early_stopping_metric = 'eval_loss'\n",
    "model_args.use_early_stopping = True\n",
    "model_args.early_stopping_delta = 0.001\n",
    "model_args.optimizer = 'AdamW'\n",
    "model_args.early_stopping_patience = 1\n",
    "model_args.do_lower_case = False\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.train_batch_size = 32 \n",
    "model_args.learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model\n",
    "model = NERModel(\n",
    "    \"bert\",\n",
    "    \"bert-base-cased\",\n",
    "    args=model_args,\n",
    "    use_cuda=False\n",
    ")\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.train_model(df_train, show_running_loss=True, eval_data=df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file3 = \"data_parquet/NER-TESTING.parquet\"\n",
    "df_test = pd.read_parquet(input_file2)\n",
    "df_test = df_test.rename(columns={'index':'sentence_id','tokens':'words'})\n",
    "df_test.set_index('sentence_id',inplace=True)\n",
    "df_test = df_test.groupby('sentence_id').agg(sentences=('words', lambda x:list(x)))\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting prediction from the model for sentences in validation dataset\n",
    "predictions, _ = model.predict(df_test.sentences, split_on_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "formatted_predictions = [\n",
    "    [list(tag.values())[0] for tag in sentence]\n",
    "    for sentence in predictions\n",
    "]\n",
    "\n",
    "# Préparer les données pour le fichier de sortie\n",
    "output_data = []\n",
    "for idx, (sentence, ner_tags) in zip(df_test.index, zip(df_test.sentences, formatted_predictions)):\n",
    "    output_data.append({\n",
    "        \"unique_id\": idx,  # Utiliser l'index original de la DataFrame\n",
    "        \"tokens\": sentence,  # Les tokens de la phrase\n",
    "        \"ner_tags\": ner_tags  # Les étiquettes extraites\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du fichier de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.makedirs(\"resultat_scibert\", exist_ok=True)\n",
    "# Sauvegarder dans un fichier JSON\n",
    "with open(\"resultat_scibert/output_predictions.jsonlines\", \"w\") as f:\n",
    "    for record in output_data:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "# Affichage pour vérification\n",
    "with open(\"resultat_scibert/output_predictions.jsonlines\", \"r\") as f:\n",
    "    for line in f:\n",
    "        print(line.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
